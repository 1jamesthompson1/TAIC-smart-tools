{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from azure.data.tables import TableServiceClient\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "# Need to set the model to use to estimate the cost of the conversations\n",
    "\n",
    "input_cost_per_million = 2\n",
    "output_cost_per_million = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ecdae",
   "metadata": {},
   "source": [
    "# Getting logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e4ba5",
   "metadata": {},
   "source": [
    "---\n",
    "üìã **Code Section: Data Loading**\n",
    "\n",
    "*This section contains Python code that:*\n",
    "- *Loads environment variables and configures Azure Table Storage connections*\n",
    "- *Retrieves chat logs and knowledge search logs from Azure tables*  \n",
    "- *Processes and cleans the data for analysis*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d914d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT_NAME')};AccountKey={os.getenv('AZURE_STORAGE_ACCOUNT_KEY')};EndpointSuffix=core.windows.net\"\n",
    "client = TableServiceClient.from_connection_string(conn_str=connection_string)\n",
    "chatlogs_table = client.get_table_client(\n",
    "    table_name=os.getenv(\"CONVERSATION_METADATA_TABLE_NAME\"),\n",
    ")\n",
    "knowledgesearch_table = client.get_table_client(\n",
    "    table_name=os.getenv(\"KNOWLEDGE_SEARCH_LOGS_TABLE_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73acdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatlogs = pd.DataFrame(list(chatlogs_table.list_entities()))\n",
    "chatlogs[\"last_updated\"] = pd.to_datetime(chatlogs[\"last_updated\"])\n",
    "chatlogs = chatlogs.sort_values(by=\"last_updated\", ascending=False)\n",
    "print(f\"Successfully loaded {len(chatlogs)} chat logs.\")\n",
    "\n",
    "# How to remove any conversation from a specific user\n",
    "chatlogs = chatlogs[chatlogs[\"PartitionKey\"] != \"James Thompson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e923d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgesearch = pd.DataFrame(\n",
    "    list(knowledgesearch_table.list_entities()),\n",
    ")\n",
    "knowledgesearch[\"search_start_time\"] = pd.to_datetime(\n",
    "    knowledgesearch[\"search_timestamp\"],\n",
    ")\n",
    "knowledgesearch = knowledgesearch.sort_values(by=\"search_start_time\", ascending=False)\n",
    "print(f\"Successfully loaded {len(knowledgesearch)} knowledge search logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859612bd",
   "metadata": {},
   "source": [
    "# Showing usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9513cc3",
   "metadata": {},
   "source": [
    "---\n",
    "üìä **Code Section: Usage Analysis & Visualization**\n",
    "\n",
    "*This section contains Python code that:*\n",
    "- *Analyzes conversation and search patterns over time*\n",
    "- *Creates daily and weekly usage trend visualizations*\n",
    "- *Identifies top users and generates summary statistics*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now(datetime.timezone.utc)\n",
    "thirty_days_ago = now - timedelta(days=30)\n",
    "ninety_days_ago = now - timedelta(days=90)\n",
    "chatlogs_30 = chatlogs[chatlogs[\"last_updated\"] >= thirty_days_ago]\n",
    "chatlogs_90 = chatlogs[chatlogs[\"last_updated\"] >= ninety_days_ago]\n",
    "knowledgesearch_30 = knowledgesearch[\n",
    "    knowledgesearch[\"search_start_time\"] >= thirty_days_ago\n",
    "]\n",
    "knowledgesearch_90 = knowledgesearch[\n",
    "    knowledgesearch[\"search_start_time\"] >= ninety_days_ago\n",
    "]\n",
    "\n",
    "# Calculate summaries\n",
    "unique_users_30 = chatlogs_30[\"PartitionKey\"].nunique()\n",
    "total_conversations_30 = len(chatlogs_30)\n",
    "total_searches_30 = len(knowledgesearch_30)\n",
    "\n",
    "unique_users_90 = chatlogs_90[\"PartitionKey\"].nunique()\n",
    "total_conversations_90 = len(chatlogs_90)\n",
    "total_searches_90 = len(knowledgesearch_90)\n",
    "\n",
    "# Print summaries\n",
    "print(\"üìä User Summary - Last 30 Days:\")\n",
    "print(f\"   Unique Users: {unique_users_30}\")\n",
    "print(f\"   Total Conversations: {total_conversations_30}\")\n",
    "print(f\"   Total Knowledge Searches: {total_searches_30}\")\n",
    "if unique_users_30 > 0:\n",
    "    print(\n",
    "        f\"   Average Conversations per User: {total_conversations_30 / unique_users_30:.1f}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"   Average Conversations per User: N/A\")\n",
    "\n",
    "print(\"\\nüìä User Summary - Last 90 Days:\")\n",
    "print(f\"   Unique Users: {unique_users_90}\")\n",
    "print(f\"   Total Conversations: {total_conversations_90}\")\n",
    "print(f\"   Total Knowledge Searches: {total_searches_90}\")\n",
    "if unique_users_90 > 0:\n",
    "    print(\n",
    "        f\"   Average Conversations per User: {total_conversations_90 / unique_users_90:.1f}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"   Average Conversations per User: N/A\")\n",
    "\n",
    "# print the top 5 users by number of conversations in the last 30 days\n",
    "top_users_30 = chatlogs_30[\"PartitionKey\"].value_counts().head(5)\n",
    "print(\"\\nüèÜ Top 5 Users by Conversations - Last 30 Days:\")\n",
    "print(top_users_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284db566",
   "metadata": {},
   "source": [
    "# Understanding token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f60049",
   "metadata": {},
   "source": [
    "---\n",
    "üî¢ **Code Section: Token Usage Analysis**\n",
    "\n",
    "*This section contains Python code that:*\n",
    "- *Loads conversation data from Azure Blob Storage*\n",
    "- *Calculates input/output token usage for each conversation*\n",
    "- *Provides detailed cost analysis and efficiency metrics*\n",
    "- *Creates comprehensive visualizations of token consumption patterns*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tiktoken\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# Set up blob storage client for conversation data\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_name = os.getenv(\"CONVERSATION_CONTAINER_NAME\")\n",
    "\n",
    "print(\"üîç Loading conversation blobs from the last 30 days...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get conversations from last 30 days using the chatlogs we already have\n",
    "thirty_days_ago = datetime.now(datetime.timezone.utc) - timedelta(days=30)\n",
    "recent_conversations = chatlogs[chatlogs[\"last_updated\"] >= thirty_days_ago]\n",
    "\n",
    "conversation_token_usage = []\n",
    "total_tokens = 0\n",
    "failed_loads = 0\n",
    "\n",
    "for _idx, row in recent_conversations.iterrows():\n",
    "    try:\n",
    "        # Construct blob name based on conversation ID\n",
    "        blob_name = f\"{row['PartitionKey']}/{row['RowKey']}.json\"\n",
    "\n",
    "        # Download the conversation blob\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, blob=blob_name,\n",
    "        )\n",
    "        conversation_data = json.loads(blob_client.download_blob().readall())\n",
    "\n",
    "        # Calculate token usage for this conversation\n",
    "        conversation_input_tokens, conversation_output_tokens = 0, 0\n",
    "        assistant_responses = 0\n",
    "\n",
    "        # Track cumulative context as conversation progresses\n",
    "        cumulative_messages = []\n",
    "\n",
    "        for message in conversation_data:\n",
    "            cumulative_messages.append(message)\n",
    "\n",
    "            if message.get(\"ai\"):\n",
    "                # From 0.3 onwards it split the logs into display and AI.\n",
    "                message = message[\"ai\"]  # noqa: PLW2901\n",
    "\n",
    "            if message.get(\"role\") == \"assistant\" and message[\"metadata\"] is None:\n",
    "                assistant_responses += 1\n",
    "\n",
    "                # Calculate tokens for entire conversation context up to this point\n",
    "                context_text = \"\"\n",
    "                for msg in cumulative_messages[\n",
    "                    :-1\n",
    "                ]:  # All messages before current assistant response\n",
    "                    context_text += f\"{msg['role']}: {msg['content']}\\n\"\n",
    "\n",
    "                # Add tokens for reading previous context\n",
    "                input_tokens = len(encoder.encode(context_text))\n",
    "\n",
    "                # Add tokens for assistant's response\n",
    "                output_tokens = len(encoder.encode(message.get(\"content\", \"\")))\n",
    "\n",
    "                conversation_input_tokens += input_tokens\n",
    "                conversation_output_tokens += output_tokens\n",
    "\n",
    "        if assistant_responses > 0:  # Only count conversations with assistant responses\n",
    "            conversation_token_usage.append(\n",
    "                {\n",
    "                    \"conversation_id\": row[\"RowKey\"],\n",
    "                    \"user_id\": row[\"PartitionKey\"],\n",
    "                    \"input_tokens\": conversation_input_tokens,\n",
    "                    \"output_tokens\": conversation_output_tokens,\n",
    "                    \"assistant_responses\": assistant_responses,\n",
    "                    \"avg_tokens_per_response\": (\n",
    "                        conversation_input_tokens + 4 * conversation_output_tokens\n",
    "                    )\n",
    "                    / assistant_responses,\n",
    "                    \"last_updated\": row[\"last_updated\"],\n",
    "                },\n",
    "            )\n",
    "            total_tokens += conversation_input_tokens + 4 * conversation_output_tokens\n",
    "\n",
    "    except Exception as e:  # noqa: BLE001, PERF203\n",
    "        print(\n",
    "            f\"‚ùå Failed to load conversation {row['RowKey']} for user {row['PartitionKey']}: {e}\",\n",
    "        )\n",
    "        failed_loads += 1\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "token_df = pd.DataFrame(conversation_token_usage)\n",
    "\n",
    "print(f\"‚úÖ Successfully analyzed {len(conversation_token_usage)} conversations\")\n",
    "print(f\"‚ùå Failed to load {failed_loads} conversations\")\n",
    "print(f\"üìä Total tokens processed: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for visualization\n",
    "\n",
    "# Add total token columns for easier analysis\n",
    "token_df[\"total_tokens\"] = token_df[\"input_tokens\"] + token_df[\"output_tokens\"]\n",
    "token_df[\"weighted_total_tokens\"] = (\n",
    "    token_df[\"input_tokens\"] + 4 * token_df[\"output_tokens\"]\n",
    ")\n",
    "\n",
    "# Calculate totals\n",
    "total_input_tokens = token_df[\"input_tokens\"].sum()\n",
    "total_output_tokens = token_df[\"output_tokens\"].sum()\n",
    "total_weighted_tokens = token_df[\"weighted_total_tokens\"].sum()\n",
    "\n",
    "print(\"\\nüìà TOKEN USAGE STATISTICS (Last 30 days)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Per-conversation statistics\n",
    "print(f\"Total conversations analyzed: {len(token_df)}\")\n",
    "print(f\"Total input tokens: {total_input_tokens:,}\")\n",
    "print(f\"Total output tokens: {total_output_tokens:,}\")\n",
    "print(f\"Total combined tokens: {total_input_tokens + total_output_tokens:,}\")\n",
    "print(f\"Input/Output ratio: {total_input_tokens / total_output_tokens:.1f}:1\")\n",
    "\n",
    "print(\"\\nPer-conversation averages:\")\n",
    "print(f\"Average input tokens per conversation: {token_df['input_tokens'].mean():.0f}\")\n",
    "print(f\"Average output tokens per conversation: {token_df['output_tokens'].mean():.0f}\")\n",
    "print(f\"Average total tokens per conversation: {token_df['total_tokens'].mean():.0f}\")\n",
    "print(f\"Median total tokens per conversation: {token_df['total_tokens'].median():.0f}\")\n",
    "print(\n",
    "    f\"Token usage range: {token_df['total_tokens'].min():,} - {token_df['total_tokens'].max():,}\",\n",
    ")\n",
    "\n",
    "# Per-response statistics\n",
    "print(\"\\nPer-response averages:\")\n",
    "print(\n",
    "    f\"Average input tokens per assistant response: {(token_df['input_tokens'] / token_df['assistant_responses']).mean():.0f}\",\n",
    ")\n",
    "print(\n",
    "    f\"Average output tokens per assistant response: {(token_df['output_tokens'] / token_df['assistant_responses']).mean():.0f}\",\n",
    ")\n",
    "\n",
    "# Conversation length and cost analysis\n",
    "print(\"\\nüí¨ CONVERSATION LENGTH ANALYSIS:\")\n",
    "print(\n",
    "    f\"Average conversation length: {token_df['assistant_responses'].mean():.1f} assistant responses\",\n",
    ")\n",
    "print(\n",
    "    f\"Median conversation length: {token_df['assistant_responses'].median():.0f} assistant responses\",\n",
    ")\n",
    "print(\n",
    "    f\"Conversation length range: {token_df['assistant_responses'].min()} - {token_df['assistant_responses'].max()} responses\",\n",
    ")\n",
    "print(f\"Most common length: {token_df['assistant_responses'].mode().iloc[0]} responses\")\n",
    "\n",
    "# Calculate per-conversation costs for analysis\n",
    "token_df[\"conversation_cost\"] = (\n",
    "    token_df[\"input_tokens\"] / 1_000_000\n",
    ") * input_cost_per_million + (\n",
    "    token_df[\"output_tokens\"] / 1_000_000\n",
    ") * output_cost_per_million\n",
    "\n",
    "print(\"\\nüí∞ CONVERSATION COST ANALYSIS:\")\n",
    "print(f\"Average cost per conversation: ${token_df['conversation_cost'].mean():.4f}\")\n",
    "print(f\"Median cost per conversation: ${token_df['conversation_cost'].median():.4f}\")\n",
    "print(\n",
    "    f\"Cost range: ${token_df['conversation_cost'].min():.4f} - ${token_df['conversation_cost'].max():.4f}\",\n",
    ")\n",
    "print(f\"Standard deviation: ${token_df['conversation_cost'].std():.4f}\")\n",
    "\n",
    "# Cost percentiles\n",
    "print(\"\\nCost percentiles:\")\n",
    "print(f\"  25th percentile: ${token_df['conversation_cost'].quantile(0.25):.4f}\")\n",
    "print(f\"  50th percentile: ${token_df['conversation_cost'].quantile(0.50):.4f}\")\n",
    "print(f\"  75th percentile: ${token_df['conversation_cost'].quantile(0.75):.4f}\")\n",
    "print(f\"  95th percentile: ${token_df['conversation_cost'].quantile(0.95):.4f}\")\n",
    "\n",
    "# Top token consumers\n",
    "print(\"\\nüî• HIGHEST TOKEN USAGE CONVERSATIONS:\")\n",
    "top_consumers = token_df.nlargest(5, \"total_tokens\")[\n",
    "    [\n",
    "        \"conversation_id\",\n",
    "        \"user_id\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"assistant_responses\",\n",
    "    ]\n",
    "]\n",
    "for _, conv in top_consumers.iterrows():\n",
    "    print(\n",
    "        f\"   {conv['conversation_id'][:8]}... ({conv['user_id']}): {conv['input_tokens']:,} in + {conv['output_tokens']:,} out = {conv['total_tokens']:,} tokens ({conv['assistant_responses']} responses)\",\n",
    "    )\n",
    "\n",
    "# User-level analysis\n",
    "user_usage = (\n",
    "    token_df.groupby(\"user_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"input_tokens\": \"sum\",\n",
    "            \"output_tokens\": \"sum\",\n",
    "            \"total_tokens\": \"sum\",\n",
    "            \"conversation_id\": \"count\",\n",
    "        },\n",
    "    )\n",
    "    .sort_values(\"total_tokens\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nüë• TOP TOKEN CONSUMERS BY USER:\")\n",
    "for user, data in user_usage.head(5).iterrows():\n",
    "    print(\n",
    "        f\"   {user}: {data['input_tokens']:,} in + {data['output_tokens']:,} out = {data['total_tokens']:,} tokens ({data['conversation_id']} conversations)\",\n",
    "    )\n",
    "\n",
    "# Calculate conversation costs and lengths for visualization\n",
    "token_df[\"conversation_cost\"] = (\n",
    "    token_df[\"input_tokens\"] / 1_000_000\n",
    ") * input_cost_per_million + (\n",
    "    token_df[\"output_tokens\"] / 1_000_000\n",
    ") * output_cost_per_million\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Distribution of conversation costs\n",
    "ax1.hist(\n",
    "    token_df[\"conversation_cost\"],\n",
    "    bins=30,\n",
    "    alpha=0.7,\n",
    "    color=\"#2e679c\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax1.set_title(\"Distribution of Conversation Costs\", fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Cost per Conversation ($)\")\n",
    "ax1.set_ylabel(\"Number of Conversations\")\n",
    "ax1.grid(visible=True, alpha=0.3)\n",
    "\n",
    "# Add statistics to the cost distribution plot\n",
    "cost_mean = token_df[\"conversation_cost\"].mean()\n",
    "cost_median = token_df[\"conversation_cost\"].median()\n",
    "ax1.axvline(\n",
    "    cost_mean, color=\"red\", linestyle=\"--\", alpha=0.8, label=f\"Mean: ${cost_mean:.4f}\",\n",
    ")\n",
    "ax1.axvline(\n",
    "    cost_median,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.8,\n",
    "    label=f\"Median: ${cost_median:.4f}\",\n",
    ")\n",
    "ax1.legend()\n",
    "\n",
    "# Distribution of conversation lengths (number of assistant responses as proxy)\n",
    "ax2.hist(\n",
    "    token_df[\"assistant_responses\"],\n",
    "    bins=range(1, max(token_df[\"assistant_responses\"]) + 2),\n",
    "    alpha=0.7,\n",
    "    color=\"#e6a832\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax2.set_title(\"Distribution of Conversation Lengths\", fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Number of Assistant Responses\")\n",
    "ax2.set_ylabel(\"Number of Conversations\")\n",
    "ax2.grid(visible=True, alpha=0.3)\n",
    "\n",
    "# Add statistics to the length distribution plot\n",
    "length_mean = token_df[\"assistant_responses\"].mean()\n",
    "length_median = token_df[\"assistant_responses\"].median()\n",
    "ax2.axvline(\n",
    "    length_mean,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.8,\n",
    "    label=f\"Mean: {length_mean:.1f}\",\n",
    ")\n",
    "ax2.axvline(\n",
    "    length_median,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.8,\n",
    "    label=f\"Median: {length_median:.0f}\",\n",
    ")\n",
    "ax2.legend()\n",
    "\n",
    "# Cost vs Conversation Length scatter plot\n",
    "ax3.scatter(\n",
    "    token_df[\"assistant_responses\"],\n",
    "    token_df[\"conversation_cost\"],\n",
    "    alpha=0.6,\n",
    "    color=\"#2e679c\",\n",
    ")\n",
    "ax3.set_title(\"Conversation Cost vs Length\", fontweight=\"bold\")\n",
    "ax3.set_xlabel(\"Number of Assistant Responses\")\n",
    "ax3.set_ylabel(\"Conversation Cost ($)\")\n",
    "ax3.grid(visible=True, alpha=0.3)\n",
    "\n",
    "# Daily token usage over time (input vs output)\n",
    "token_df[\"date\"] = token_df[\"last_updated\"].dt.date\n",
    "daily_input_tokens = token_df.groupby(\"date\")[\"input_tokens\"].sum()\n",
    "daily_output_tokens = token_df.groupby(\"date\")[\"output_tokens\"].sum()\n",
    "\n",
    "# Align the indices to handle any missing dates\n",
    "all_dates = daily_input_tokens.index.union(daily_output_tokens.index)\n",
    "daily_input_tokens = daily_input_tokens.reindex(all_dates, fill_value=0)\n",
    "daily_output_tokens = daily_output_tokens.reindex(all_dates, fill_value=0)\n",
    "\n",
    "ax4.fill_between(\n",
    "    daily_input_tokens.index,\n",
    "    daily_input_tokens.to_numpy(),\n",
    "    alpha=0.7,\n",
    "    color=\"#2e679c\",\n",
    "    label=\"Input Tokens\",\n",
    ")\n",
    "ax4.fill_between(\n",
    "    daily_output_tokens.index,\n",
    "    daily_input_tokens.to_numpy(),\n",
    "    daily_input_tokens.to_numpy() + daily_output_tokens.to_numpy(),\n",
    "    alpha=0.7,\n",
    "    color=\"#e6a832\",\n",
    "    label=\"Output Tokens\",\n",
    ")\n",
    "ax4.set_title(\"Daily Token Usage (Input vs Output)\", fontweight=\"bold\")\n",
    "ax4.set_xlabel(\"Date\")\n",
    "ax4.set_ylabel(\"Total Tokens\")\n",
    "ax4.legend()\n",
    "ax4.grid(visible=True, alpha=0.3)\n",
    "ax4.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed cost estimation based on actual input/output pricing\n",
    "print(\"\\nüí∞ DETAILED COST ESTIMATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Calculate costs\n",
    "input_cost = (total_input_tokens / 1_000_000) * input_cost_per_million\n",
    "output_cost = (total_output_tokens / 1_000_000) * output_cost_per_million\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(\"\\nToken breakdown:\")\n",
    "print(\n",
    "    f\"  Input tokens: {total_input_tokens:,} @ ${input_cost_per_million}/1M = ${input_cost:.4f}\",\n",
    ")\n",
    "print(\n",
    "    f\"  Output tokens: {total_output_tokens:,} @ ${output_cost_per_million}/1M = ${output_cost:.4f}\",\n",
    ")\n",
    "print(f\"  Total cost (30 days): ${total_cost:.4f}\")\n",
    "print(f\"  Daily average: ${total_cost / 30:.4f}\")\n",
    "print(f\"  Estimated monthly cost: ${total_cost:.4f}\")\n",
    "\n",
    "# Cost breakdown by user\n",
    "print(\"\\nüí≥ COST BY TOP USERS:\")\n",
    "for user, data in user_usage.head(5).iterrows():\n",
    "    user_input_cost = (data[\"input_tokens\"] / 1_000_000) * input_cost_per_million\n",
    "    user_output_cost = (data[\"output_tokens\"] / 1_000_000) * output_cost_per_million\n",
    "    user_total_cost = user_input_cost + user_output_cost\n",
    "    print(\n",
    "        f\"   {user}: ${user_total_cost:.4f} (${user_input_cost:.4f} input + ${user_output_cost:.4f} output)\",\n",
    "    )\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\nüìä EFFICIENCY INSIGHTS:\")\n",
    "print(f\"Average input/output ratio: {total_input_tokens / total_output_tokens:.1f}:1\")\n",
    "print(f\"Cost per conversation: ${total_cost / len(token_df):.4f}\")\n",
    "print(\n",
    "    f\"Most expensive conversation: ${(top_consumers.iloc[0]['input_tokens'] / 1_000_000) * input_cost_per_million + (top_consumers.iloc[0]['output_tokens'] / 1_000_000) * output_cost_per_million:.4f}\",\n",
    ")\n",
    "print(\n",
    "    f\"Cheapest conversation: ${(token_df['input_tokens'].min() / 1_000_000) * input_cost_per_million + (token_df['output_tokens'].min() / 1_000_000) * output_cost_per_million:.4f}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-smart-tools (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
